<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-08-09T12:43:38-04:00</updated><id>/feed.xml</id><title type="html">rfdavid</title><subtitle>AI | Machine Learning | Graph Neural Networks </subtitle><entry><title type="html">Paper review - Design Space for Graph Neural Networks</title><link href="/design-space-for-gnn/" rel="alternate" type="text/html" title="Paper review - Design Space for Graph Neural Networks" /><published>2021-12-20T10:27:31-05:00</published><updated>2021-12-20T10:27:31-05:00</updated><id>/design-space-for-gnn</id><content type="html" xml:base="/design-space-for-gnn/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.08843.pdf&quot;&gt;Design Space for Graph Neural Networks&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#you2020design&quot;&gt;(You et al., 2020)&lt;/a&gt;
was published on NeurIPS 2020. The authors are Jiaxuan You, Zhitao Ying and Jure Leskovec
from Stanford. There is also a very good video from the author &lt;a href=&quot;https://www.youtube.com/watch?v=8OhnwzT9ypg&quot;&gt;available on
YouTube&lt;/a&gt;. 
The code is also available on &lt;a href=&quot;https://github.com/snap-stanford/graphgym&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Instead of evaluating a specific architecture of GNNs such as GCN, GIN or GAT,
the paper explores the design space in a more general way. For example, is
batch normalization helpful in GNNs? This paper answer this question
empirically by performing multiple experiments.&lt;/p&gt;

&lt;p&gt;The paper takes a systematic approach to study a general design space of GNN for
many different tasks, presenting three key innovations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;General GNN design space&lt;/li&gt;
  &lt;li&gt;GNN task space with a similarity metric&lt;/li&gt;
  &lt;li&gt;Design space evaluation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;general-gnn-design-space&quot;&gt;General GNN design space&lt;/h3&gt;

&lt;p&gt;The design space is based on three configurations: intra-layer design, inter-layer design,
and learning configuration. All combined possibilities result in 314,928
different designs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/gnn-design-space.png&quot; alt=&quot;medium&quot; title=&quot;GNN design space&quot; /&gt;
&lt;em&gt;Figure 1: General design space divided into intra-layer, inter-layer and
learning configuration. Image extracted from &lt;a class=&quot;citation&quot; href=&quot;#you2020design&quot;&gt;(You et al., 2020)&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Intra-layer&lt;/strong&gt; design follows the sequence of the modules:&lt;/p&gt;

\[h^{k+1}_{v} = AGG\Big(\Big\{ACT\Big(DROPOUT(BN(W^{(k)}*h_u^{(k)} + b^{(k)}))\Big) \Big\}, u \in \mathcal{N}(v)\Big)\]

&lt;p&gt;It uses the following ranges:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Aggregation                  &lt;/th&gt;
      &lt;th&gt;Activation                           &lt;/th&gt;
      &lt;th&gt;Dropout                 &lt;/th&gt;
      &lt;th&gt;Batch Normalization&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Mean, Max, Sum&lt;/td&gt;
      &lt;td&gt;ReLU, PReLU, Swish&lt;/td&gt;
      &lt;td&gt;False, 0.3, 0.6&lt;/td&gt;
      &lt;td&gt;True, False&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inter-layer&lt;/strong&gt; design is the neural network layers:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Layer connectivity                  &lt;/th&gt;
      &lt;th&gt;Pre-process layers       &lt;/th&gt;
      &lt;th&gt;Message passing layers    &lt;/th&gt;
      &lt;th&gt;Post-precess layers&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Stack, Skip-Sum, Skip-Cat&lt;/td&gt;
      &lt;td&gt;1, 2, 3&lt;/td&gt;
      &lt;td&gt;2, 4, 6, 8&lt;/td&gt;
      &lt;td&gt;1, 2, 3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training configuration&lt;/strong&gt; is the configuration:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Batch size                  &lt;/th&gt;
      &lt;th&gt;Learning rate                           &lt;/th&gt;
      &lt;th&gt;Optmizer                &lt;/th&gt;
      &lt;th&gt;Training epochs&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;16, 32, 64&lt;/td&gt;
      &lt;td&gt;0.1, 0.01, 0.001&lt;/td&gt;
      &lt;td&gt;SGD, Adam&lt;/td&gt;
      &lt;td&gt;100, 200, 400&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I believe some of the properties selected above should not be labelled as
architecture (i.e. learning rate, epochs). The &lt;a href=&quot;https://www.youtube.com/watch?v=5ke9ZEvXJEk&quot;&gt;talk by Ameet
Talkwalkar&lt;/a&gt; well address the
difference between hyper-parameter search and neural architecture search. 
Hyperparameter search starts assuming you have a fixed neural network backbone, 
and then there are certain properties that you want to tune.
Some properties are architectural and others non-architectural:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Architectural&lt;/strong&gt;: nodes per layer, number of layers, activation function&lt;br /&gt;
&lt;strong&gt;Non-architectural&lt;/strong&gt;: regularization, learning rate, batch size&lt;/p&gt;

&lt;p&gt;In NAS, you ignore the non-architectural parameters, and you also consider layer
operations and networks connections in the architectural setting. 
Hyperparameter is the entire space to build your network, whereas neural architecture search
is limited by a defined design space.&lt;/p&gt;

&lt;h3 id=&quot;gnn-task-space-with-a-similarity-metric&quot;&gt;GNN task space with a similarity metric&lt;/h3&gt;

&lt;p&gt;The paper developed a technique to measure and quantify the GNN task space in 
conjunction with the design space.
This is the most interesting idea from this paper, in my opinion, and could
spawn other promising ideas. 
They collect 32 synthetic and real-world GNN tasks/datasets and use Kendall
rank correlation &lt;a class=&quot;citation&quot; href=&quot;#abdi2007kendall&quot;&gt;(Abdi, 2007)&lt;/a&gt; to compare an evaluated task to a
new task. The finding is very interesting: similar tasks perform well using
similar configurations, and the inverse is true. The implication is the
possibility of transferring the configuration from one known task to a new
task/dataset.&lt;/p&gt;

&lt;p&gt;The example below demonstrates two different tasks, A and B. A controlled random
search is applied to find the best design performance for each task. In this
example, task A performed better using sum aggregation function, whereas task B
performed better using max aggregation function. The question is if it’s
possible to use the same configuration to a new similar task based on
similarity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/task-transfer.png&quot; alt=&quot;medium&quot; title=&quot;Task similarity example&quot; /&gt;
&lt;em&gt;Table 1: Image extracted from &lt;a class=&quot;citation&quot; href=&quot;#you2020design&quot;&gt;(You et al., 2020)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Once introducing a new target task (ogbg-molhiv in the example), a task similarity 
is calculated. Task A has a correlation of 0.47, and Task B has a negative
correlation of -0.61. When testing both configurations from A and B to the new
task, the performance was significantly better using Task A design which has a
high correlation with the target task.&lt;/p&gt;

&lt;h3 id=&quot;design-space-evaluation&quot;&gt;Design space evaluation&lt;/h3&gt;

&lt;p&gt;The evaluation of design space alongside all the tasks lead to over 10 million
possible combinations. A controlled random search is proposed to explore this
space. It basically randomly sample 96 setups out of the 10M possibilities,
control the configuration to be tested and evaluated. For example, consider
batch normalization as the target study. A sample of 96 different
configurations is randomly sampled among the design space. Batch
normalization is set to True and evaluated. By preserving the other parameters,
batch normalization is set to False and then evaluated again. The results are
ranked by performance to generate a distribution, and the frequency is used to
analyze whether batch normalization is generally helpful or not.&lt;/p&gt;

&lt;h2 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h2&gt;

&lt;p&gt;The paper show a nice visualization using violin plot for the experiments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/design-space-results.png&quot; alt=&quot;&quot; title=&quot;GNN design space results&quot; /&gt;
&lt;em&gt;Figure 3: Boxplot of the results. Image extracted from &lt;a class=&quot;citation&quot; href=&quot;#you2020design&quot;&gt;(You et al., 2020)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Each plot represents the distribution of the rank. For example, the first graph
is the distribution of the experiments for batch normalization. By evaluating
different architectures randomly, when setting batch normalization to True, it
ranked better (lower is better), indicating that in most cases, the GNN will
perform better when this property is used.
The most expressive configurations found in this paper are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dropout node feature is not effective.&lt;/li&gt;
  &lt;li&gt;PReLU stands out as the choice of activation.&lt;/li&gt;
  &lt;li&gt;Sum aggregation is the most expressive.&lt;/li&gt;
  &lt;li&gt;There is no definitive conclusion for the number of message passing layers,
pre-processing layers or pos-processing layers.&lt;/li&gt;
  &lt;li&gt;Skip connections are generally favorable.&lt;/li&gt;
  &lt;li&gt;Batch size of 32 is a safer choice, as learning rate of 0.01.&lt;/li&gt;
  &lt;li&gt;ADAM resulted in better performance than SGD.&lt;/li&gt;
  &lt;li&gt;More epochs of training lead to better performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;you2020design&quot;&gt;You, J., Ying, R., &amp;amp; Leskovec, J. (2020). Design Space for Graph Neural Networks. &lt;i&gt;NeurIPS&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;abdi2007kendall&quot;&gt;Abdi, H. (2007). &lt;i&gt;The kendall rank correlation coefficient&lt;/i&gt;. Encyclopedia of Measurement and Statistics.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Rui F. David</name></author><category term="paper" /><summary type="html">Introduction</summary></entry><entry><title type="html">What is this about</title><link href="/first-post/" rel="alternate" type="text/html" title="What is this about" /><published>2021-10-16T13:27:31-04:00</published><updated>2021-10-16T13:27:31-04:00</updated><id>/first-post</id><content type="html" xml:base="/first-post/">&lt;p&gt;I intend to share my ideas here in this blog soon. For now, I’m getting more familiar with Jekyll. I have many private writings, technical and non-technicals that I would love to share shortly.&lt;/p&gt;</content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html">I intend to share my ideas here in this blog soon. For now, I’m getting more familiar with Jekyll. I have many private writings, technical and non-technicals that I would love to share shortly.</summary></entry></feed>