---
---
References
==========

@InProceedings{you2020design,
  title = {Design Space for Graph Neural Networks},
  author = {You, Jiaxuan and Ying, Rex and Leskovec, Jure},
  booktitle = {NeurIPS},
  year = {2020}
}

@book{abdi2007kendall,
  title     = {The kendall rank correlation coefficient},
  author    = {H. Abdi},
  year      = {2007},
  publisher = {Encyclopedia of Measurement and Statistics.}
}

@article{10.2307/2285666,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2285666},
 abstract = {This paper treats essentially the first derivative of an estimator viewed as functional and the ways in which it can be used to study local robustness properties. A theory of robust estimation "near" strict parametric models is briefly sketched and applied to some classical situations. Relations between von Mises functionals, the jackknife and U-statistics are indicated. A number of classical and new estimators are discussed, including trimmed and Winsorized means, Huber-estimators, and more generally maximum likelihood and M-estimators. Finally, a table with some numerical robustness properties is given.},
 author = {Frank R. Hampel},
 journal = {Journal of the American Statistical Association},
 number = {346},
 pages = {383--393},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {The Influence Curve and Its Role in Robust Estimation},
 urldate = {2022-07-27},
 volume = {69},
 year = {1974}
}

@InProceedings{pmlr-v70-koh17a,
  title = 	 {Understanding Black-box Predictions via Influence Functions},
  author =       {Pang Wei Koh and Percy Liang},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1885--1894},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/koh17a/koh17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/koh17a.html},
  abstract = 	 {How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.}
}

@article{JMLR:v18:16-491,
  author  = {Naman Agarwal and Brian Bullins and Elad Hazan},
  title   = {Second-Order Stochastic Optimization for Machine Learning in Linear Time},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {116},
  pages   = {1--40},
  url     = {http://jmlr.org/papers/v18/16-491.html}
}

@inproceedings{10.5555/3104322.3104416,
author = {Martens, James},
title = {Deep Learning via Hessian-Free Optimization},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We develop a 2nd-order optimization method based on the "Hessian-free" approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton & Salakhutdinov (2006) on the same tasks they considered. Our method is practical, easy to use, scales nicely to very large datasets, and isn't limited in applicability to auto-encoders, or any specific model class. We also discuss the issue of "pathological curvature" as a possible explanation for the difficulty of deep-learning and how 2nd-order optimization, and our method in particular, effectively deals with it.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {735–742},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@book{maronna2006robust,
  title={Robust Statistics: Theory and Methods},
  author={Maronna, R.A. and Martin, D.R. and Yohai, V.J.},
  isbn={9780470010921},
  series={Wiley Series in Probability and Statistics},
  url={https://books.google.ca/books?id=iFVjQgAACAAJ},
  year={2006},
  publisher={Wiley}
}

@book{cook1982influence,
  title={Residuals and Influence in Regression },
  author={Cook, R. Dennis, and Sanford Weisberg},
  isbn={041224280X},
  series={Monographs on statistics and applied probability},
  url={https://franklin.library.upenn.edu/catalog/FRANKLIN_991342763503681},
  year={1982},
  publisher={New York: Chapman and Hall}
}
